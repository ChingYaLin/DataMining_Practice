{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18c1af90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler # for scaling the features.\n",
    "# Association rules\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree # for decision tree\n",
    "from sklearn.ensemble import RandomForestClassifier # for random forest\n",
    "from sklearn.metrics import classification_report, confusion_matrix, silhouette_score\n",
    "from sklearn.metrics import roc_curve, auc # plot the ROC curve to compre the classification result\n",
    "\n",
    "# Heirarchical clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "# Kmeans clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "\n",
    "# For estimate the clustering results.\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66e990af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Functions\n",
    "# For Standardize\n",
    "def standardize_features(df, feature_cols):\n",
    "    \"\"\"\n",
    "    Normalize the data.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler() # normalization setting\n",
    "    X_scaled = scaler.fit_transform(df[feature_cols].values)\n",
    "    df_scaled = pd.DataFrame(X_scaled, columns=feature_cols, index=df.index)\n",
    "    return df_scaled, scaler\n",
    "\n",
    "# For Association Rule\n",
    "def discretize_for_association(df, bins = 3):\n",
    "    \"\"\"\n",
    "    Discretize continuous variables into low/medium/high labels\n",
    "    for association rules. \n",
    "    strategy: \"uniform\" uses cut with equal-width bins.\n",
    "    Returns a one-hot encoded boolean DataFrame.\n",
    "    \"\"\"\n",
    "    labels = [\"low\", \"medium\", \"high\"][:bins]   # 避免 bins !=3 時報錯\n",
    "    disc = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        series = df[col]\n",
    "        try:\n",
    "            cats = pd.cut(series, bins=bins, labels=labels)\n",
    "        except ValueError:\n",
    "            # Fallback: if too few unique values, treat as is\n",
    "            nbins = min(bins, max(2, series.nunique()))\n",
    "            fallback_labels = labels[:nbins]\n",
    "            cats = pd.cut(series, bins=nbins, labels=fallback_labels, duplicates=\"drop\")\n",
    "        disc[col] = cats.astype(str)\n",
    "\n",
    "    disc_df = pd.DataFrame(disc, index=df.index)\n",
    "\n",
    "    # One-hot encoding for mlxtend\n",
    "    ohe_parts = []\n",
    "    for col in disc_df.columns:\n",
    "        dummies = pd.get_dummies(disc_df[col], prefix=col, dtype=bool)\n",
    "        ohe_parts.append(dummies)\n",
    "\n",
    "    return pd.concat(ohe_parts, axis=1)\n",
    "\n",
    "\n",
    "def run_association_rules(ohe_df, min_support = 0.1,\n",
    "                          min_confidence = 0.5, min_lift = 1.0):\n",
    "    freq = apriori(ohe_df.astype(bool), min_support=min_support, use_colnames=True)\n",
    "    if freq.empty:\n",
    "        return pd.DataFrame()\n",
    "    rules = association_rules(freq, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    rules = rules[rules['antecedents'].apply(lambda x: len(x) <= 1)]\n",
    "    rules = rules[rules['consequents'].apply(lambda x: len(x) == 1)]\n",
    "    if \"lift\" in rules.columns:\n",
    "        rules = rules[rules[\"lift\"] >= min_lift]\n",
    "    # Sort by confidence then lift\n",
    "    rules = rules.sort_values([\"confidence\", \"lift\"], ascending=False)\n",
    "    return rules.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51f8a5a",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "718ae429",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"/Users/ching-yalin/Desktop/TIGP/2025_Fall/Biological Computing (C1)/2025-10-27_Introduction to Data Mining (Dr. Henry Horng-Shing Lu)/outputs/\"\n",
    "#### Data loading\n",
    "wine_df = pd.read_csv(\"/Users/ching-yalin/Desktop/TIGP/2025_Fall/Biological Computing (C1)/2025-10-27_Introduction to Data Mining (Dr. Henry Horng-Shing Lu)/wine+quality/winequality-red.csv\", sep = ';')\n",
    "#### Standardization\n",
    "feature_col = [col for col in wine_df if col != 'quality'] # quality is final predicted results\n",
    "scaled_df, scaler = standardize_features(wine_df, feature_col)\n",
    "scaled_df.head()\n",
    "# Note: the scaled data only for PCA and clustering\n",
    "quality_threshold = 6 # The wine_quality threshold for good/bad label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab98edfd",
   "metadata": {},
   "source": [
    "### Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5195a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>representativity</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>zhangs_metric</th>\n",
       "      <th>jaccard</th>\n",
       "      <th>certainty</th>\n",
       "      <th>kulczynski</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(alcohol_medium)</td>\n",
       "      <td>(quality_good)</td>\n",
       "      <td>0.349593</td>\n",
       "      <td>0.534709</td>\n",
       "      <td>0.275172</td>\n",
       "      <td>0.787120</td>\n",
       "      <td>1.472052</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.088241</td>\n",
       "      <td>2.185694</td>\n",
       "      <td>0.493040</td>\n",
       "      <td>0.451745</td>\n",
       "      <td>0.542479</td>\n",
       "      <td>0.650870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(density_low)</td>\n",
       "      <td>(quality_good)</td>\n",
       "      <td>0.106942</td>\n",
       "      <td>0.534709</td>\n",
       "      <td>0.083177</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.454581</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.025994</td>\n",
       "      <td>2.093809</td>\n",
       "      <td>0.349940</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.522401</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(citric acid_medium)</td>\n",
       "      <td>(quality_good)</td>\n",
       "      <td>0.347092</td>\n",
       "      <td>0.534709</td>\n",
       "      <td>0.227017</td>\n",
       "      <td>0.654054</td>\n",
       "      <td>1.223196</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.041424</td>\n",
       "      <td>1.344981</td>\n",
       "      <td>0.279472</td>\n",
       "      <td>0.346705</td>\n",
       "      <td>0.256495</td>\n",
       "      <td>0.539308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(volatile acidity_medium)</td>\n",
       "      <td>(quality_bad)</td>\n",
       "      <td>0.308318</td>\n",
       "      <td>0.465291</td>\n",
       "      <td>0.200125</td>\n",
       "      <td>0.649087</td>\n",
       "      <td>1.395014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.056668</td>\n",
       "      <td>1.523767</td>\n",
       "      <td>0.409381</td>\n",
       "      <td>0.348964</td>\n",
       "      <td>0.343732</td>\n",
       "      <td>0.539597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(alcohol_low)</td>\n",
       "      <td>(quality_bad)</td>\n",
       "      <td>0.616010</td>\n",
       "      <td>0.465291</td>\n",
       "      <td>0.385241</td>\n",
       "      <td>0.625381</td>\n",
       "      <td>1.344064</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098617</td>\n",
       "      <td>1.427340</td>\n",
       "      <td>0.666653</td>\n",
       "      <td>0.553459</td>\n",
       "      <td>0.299396</td>\n",
       "      <td>0.726669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 antecedents     consequents  antecedent support  \\\n",
       "0           (alcohol_medium)  (quality_good)            0.349593   \n",
       "1              (density_low)  (quality_good)            0.106942   \n",
       "2       (citric acid_medium)  (quality_good)            0.347092   \n",
       "3  (volatile acidity_medium)   (quality_bad)            0.308318   \n",
       "4              (alcohol_low)   (quality_bad)            0.616010   \n",
       "\n",
       "   consequent support   support  confidence      lift  representativity  \\\n",
       "0            0.534709  0.275172    0.787120  1.472052               1.0   \n",
       "1            0.534709  0.083177    0.777778  1.454581               1.0   \n",
       "2            0.534709  0.227017    0.654054  1.223196               1.0   \n",
       "3            0.465291  0.200125    0.649087  1.395014               1.0   \n",
       "4            0.465291  0.385241    0.625381  1.344064               1.0   \n",
       "\n",
       "   leverage  conviction  zhangs_metric   jaccard  certainty  kulczynski  \n",
       "0  0.088241    2.185694       0.493040  0.451745   0.542479    0.650870  \n",
       "1  0.025994    2.093809       0.349940  0.148936   0.522401    0.466667  \n",
       "2  0.041424    1.344981       0.279472  0.346705   0.256495    0.539308  \n",
       "3  0.056668    1.523767       0.409381  0.348964   0.343732    0.539597  \n",
       "4  0.098617    1.427340       0.666653  0.553459   0.299396    0.726669  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Association Rules\n",
    "df_for_assoc = wine_df.copy()\n",
    "df_for_assoc['quality_bin'] = np.where(df_for_assoc['quality'] >= quality_threshold,'good','bad')\n",
    "# Create one-hot encoding table for quality\n",
    "ohe_quality = pd.get_dummies(df_for_assoc['quality_bin'],prefix='quality',dtype=bool)\n",
    "# Create one-hot encoding table for other features\n",
    "ohe_features = discretize_for_association(df_for_assoc[feature_col],bins=3)\n",
    "# Create one-hot encoding table for association rule analysis\n",
    "ohe = pd.concat([ohe_features, ohe_quality], axis=1)\n",
    "rules_all = run_association_rules(ohe, min_support=0.08, min_confidence=0.5, min_lift=1.0)\n",
    "# Remain the quality association results from all rules\n",
    "quality_rules = rules_all[rules_all['consequents'].apply(lambda s: ('quality_good' in s) or ('quality_bad' in s))]\n",
    "# Prevent quality appear in antecedents because we only want to see feature -> quality\n",
    "quality_rules = quality_rules[~quality_rules['antecedents'].apply(lambda s: any(str(item).startswith('quality_') for item in s))]\n",
    "quality_rules = quality_rules.reset_index(drop=True)\n",
    "quality_rules.to_csv( output + \"assoc_rules.csv\", index=False)\n",
    "quality_rules.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c44de27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support-Confidnece-Lift Scatter plot\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(quality_rules['support'], quality_rules['confidence'],\n",
    "            c=quality_rules['lift'], cmap='viridis', s=quality_rules['lift']*60)\n",
    "\n",
    "plt.colorbar(label='Lift')\n",
    "plt.xlabel('Support')\n",
    "plt.ylabel('Confidence')\n",
    "plt.title('Association Rules: Support vs Confidence (colored by Lift)')\n",
    "plt.savefig(os.path.join(output, \"assoc_rules_support-confidence-lift.png\"), dpi=150)\n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0270fcf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>representativity</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>zhangs_metric</th>\n",
       "      <th>jaccard</th>\n",
       "      <th>certainty</th>\n",
       "      <th>kulczynski</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(alcohol_medium)</td>\n",
       "      <td>(quality_good)</td>\n",
       "      <td>0.349593</td>\n",
       "      <td>0.534709</td>\n",
       "      <td>0.275172</td>\n",
       "      <td>0.787120</td>\n",
       "      <td>1.472052</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.088241</td>\n",
       "      <td>2.185694</td>\n",
       "      <td>0.493040</td>\n",
       "      <td>0.451745</td>\n",
       "      <td>0.542479</td>\n",
       "      <td>0.650870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(density_low)</td>\n",
       "      <td>(quality_good)</td>\n",
       "      <td>0.106942</td>\n",
       "      <td>0.534709</td>\n",
       "      <td>0.083177</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.454581</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.025994</td>\n",
       "      <td>2.093809</td>\n",
       "      <td>0.349940</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.522401</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(volatile acidity_medium)</td>\n",
       "      <td>(quality_bad)</td>\n",
       "      <td>0.308318</td>\n",
       "      <td>0.465291</td>\n",
       "      <td>0.200125</td>\n",
       "      <td>0.649087</td>\n",
       "      <td>1.395014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.056668</td>\n",
       "      <td>1.523767</td>\n",
       "      <td>0.409381</td>\n",
       "      <td>0.348964</td>\n",
       "      <td>0.343732</td>\n",
       "      <td>0.539597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(alcohol_low)</td>\n",
       "      <td>(quality_bad)</td>\n",
       "      <td>0.616010</td>\n",
       "      <td>0.465291</td>\n",
       "      <td>0.385241</td>\n",
       "      <td>0.625381</td>\n",
       "      <td>1.344064</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.098617</td>\n",
       "      <td>1.427340</td>\n",
       "      <td>0.666653</td>\n",
       "      <td>0.553459</td>\n",
       "      <td>0.299396</td>\n",
       "      <td>0.726669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(citric acid_medium)</td>\n",
       "      <td>(quality_good)</td>\n",
       "      <td>0.347092</td>\n",
       "      <td>0.534709</td>\n",
       "      <td>0.227017</td>\n",
       "      <td>0.654054</td>\n",
       "      <td>1.223196</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.041424</td>\n",
       "      <td>1.344981</td>\n",
       "      <td>0.279472</td>\n",
       "      <td>0.346705</td>\n",
       "      <td>0.256495</td>\n",
       "      <td>0.539308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 antecedents     consequents  antecedent support  \\\n",
       "0           (alcohol_medium)  (quality_good)            0.349593   \n",
       "1              (density_low)  (quality_good)            0.106942   \n",
       "3  (volatile acidity_medium)   (quality_bad)            0.308318   \n",
       "4              (alcohol_low)   (quality_bad)            0.616010   \n",
       "2       (citric acid_medium)  (quality_good)            0.347092   \n",
       "\n",
       "   consequent support   support  confidence      lift  representativity  \\\n",
       "0            0.534709  0.275172    0.787120  1.472052               1.0   \n",
       "1            0.534709  0.083177    0.777778  1.454581               1.0   \n",
       "3            0.465291  0.200125    0.649087  1.395014               1.0   \n",
       "4            0.465291  0.385241    0.625381  1.344064               1.0   \n",
       "2            0.534709  0.227017    0.654054  1.223196               1.0   \n",
       "\n",
       "   leverage  conviction  zhangs_metric   jaccard  certainty  kulczynski  \n",
       "0  0.088241    2.185694       0.493040  0.451745   0.542479    0.650870  \n",
       "1  0.025994    2.093809       0.349940  0.148936   0.522401    0.466667  \n",
       "3  0.056668    1.523767       0.409381  0.348964   0.343732    0.539597  \n",
       "4  0.098617    1.427340       0.666653  0.553459   0.299396    0.726669  \n",
       "2  0.041424    1.344981       0.279472  0.346705   0.256495    0.539308  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Top 5 rules\n",
    "quality_rules.sort_values(\"lift\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5c3b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph network\n",
    "G = nx.DiGraph()\n",
    "\n",
    "for _, row in quality_rules.iterrows():\n",
    "    for a in row['antecedents']:\n",
    "        for c in row['consequents']:\n",
    "            G.add_edge(a, c, weight=row['lift'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "pos = nx.spring_layout(G, k=0.9)\n",
    "nx.draw(G, pos, with_labels=True, node_size=2200,\n",
    "        node_color='skyblue', arrowsize=20, font_size=10)\n",
    "plt.title(\"Association Rules Network Graph\")\n",
    "plt.savefig(os.path.join(output, \"assoc_rules_network_graph.png\"), dpi=150)\n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31d67c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lift Heatmap\n",
    "pivot = quality_rules.pivot_table(\n",
    "    values='lift',\n",
    "    index=quality_rules['antecedents'].astype(str),\n",
    "    columns=quality_rules['consequents'].astype(str)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(pivot, annot=False, cmap='coolwarm')\n",
    "plt.title(\"Lift Heatmap\")\n",
    "plt.savefig(os.path.join(output, \"assoc_rules_lift heatmap.png\"), dpi=150)\n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c209c4a0",
   "metadata": {},
   "source": [
    "-------\n",
    "## Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6275d3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data and testing data\n",
    "df_cls = wine_df.copy() # Use original data (not scaled)\n",
    "df_cls[\"high_quality\"] = (df_cls[\"quality\"] >= quality_threshold).astype(int) # Turn Ture to 1, False to 0\n",
    "target_col = \"high_quality\"\n",
    "\n",
    "\n",
    "x = df_cls[feature_col].values\n",
    "y = df_cls[target_col].values\n",
    "\n",
    "test_size = 0.2\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=test_size, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b72944b",
   "metadata": {},
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea014496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_decision_tree(x_train, x_test, y_train, y_test ,test_size, random_state = 42, max_depth = None):\n",
    "    # Create the decision tree classifier and apply to the traning data.\n",
    "    clf = DecisionTreeClassifier(random_state=random_state, max_depth=max_depth, criterion=\"entropy\") # Here the important of feature using entropy-based information gain\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(x_test)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0) # create the performance report\n",
    "    cm = confusion_matrix(y_test, y_pred) # get confusion matrix\n",
    "\n",
    "    results = {\n",
    "        \"classification_report\": report,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"feature_importances\": clf.feature_importances_,\n",
    "        \"test_size\": test_size,\n",
    "    }\n",
    "    return clf, results\n",
    "\n",
    "\n",
    "#### Decision tree\n",
    "clf, dt_results = run_decision_tree(x_train, x_test, y_train, y_test,\n",
    "                                        test_size=0.2,\n",
    "                                        random_state=42,\n",
    "                                        max_depth=None)\n",
    "# Save report\n",
    "rep = dt_results[\"classification_report\"]\n",
    "cm = dt_results[\"confusion_matrix\"]\n",
    "fi = dt_results[\"feature_importances\"]\n",
    "\n",
    "with open(os.path.join(output, \"decision_tree_report.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== Classification Report ===\\n\")\n",
    "    f.write(pd.DataFrame(rep).T.round(3).to_string())\n",
    "    f.write(\"\\n\\n=== Confusion Matrix ===\\n\")\n",
    "    f.write(pd.DataFrame(cm).to_string())\n",
    "    f.write(\"\\n\\n=== Feature Importances ===\\n\")\n",
    "    f.write(pd.Series(fi, index=feature_col).sort_values(ascending=False).round(4).to_string())\n",
    "\n",
    "# Save a quick PNG plot of the tree\n",
    "plt.figure(figsize=(16, 8))\n",
    "plot_tree(clf, feature_names=feature_col, class_names=[\"low\", \"high\"], filled=True, max_depth=3, fontsize=8)\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "plt.savefig(os.path.join(output, \"decision_tree.png\"), dpi=150)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bb65fe",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b522b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_forest(x_train, x_test, y_train, y_test, test_size,\n",
    "                      random_state=42, n_estimators=200, max_depth=None):\n",
    "\n",
    "    # Build Random Forest classifier\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=random_state, criterion=\"entropy\"\n",
    "    )\n",
    "    rf.fit(x_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = rf.predict(x_test)\n",
    "\n",
    "    # Metrics\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    results = {\n",
    "        \"classification_report\": report,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"feature_importances\": rf.feature_importances_,\n",
    "        \"test_size\": test_size\n",
    "    }\n",
    "    return rf, results\n",
    "\n",
    "#### Random Forest\n",
    "rf_clf, rf_results = run_random_forest(\n",
    "    x_train, x_test, y_train, y_test,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    n_estimators=200,\n",
    "    max_depth=None\n",
    ")\n",
    "\n",
    "# Save results\n",
    "rf_rep = rf_results[\"classification_report\"]\n",
    "rf_cm = rf_results[\"confusion_matrix\"]\n",
    "rf_fi = rf_results[\"feature_importances\"]\n",
    "\n",
    "with open(os.path.join(output, \"random_forest_report.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== Classification Report ===\\n\")\n",
    "    f.write(pd.DataFrame(rf_rep).T.round(3).to_string())\n",
    "    f.write(\"\\n\\n=== Confusion Matrix ===\\n\")\n",
    "    f.write(pd.DataFrame(rf_cm).to_string())\n",
    "    f.write(\"\\n\\n=== Feature Importances ===\\n\")\n",
    "    f.write(pd.Series(rf_fi, index=feature_col).sort_values(ascending=False).round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9b6397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Compare two classification model\n",
    "# Decision Tree\n",
    "dt_probs = clf.predict_proba(x_test)[:, 1]   # 機率分數（class 1）\n",
    "dt_fpr, dt_tpr, _ = roc_curve(y_test, dt_probs)\n",
    "dt_auc = auc(dt_fpr, dt_tpr)\n",
    "\n",
    "# Random Forest\n",
    "rf_probs = rf_clf.predict_proba(x_test)[:, 1]\n",
    "rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probs)\n",
    "rf_auc = auc(rf_fpr, rf_tpr)\n",
    "\n",
    "# ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(dt_fpr, dt_tpr, label=f\"Decision Tree (AUC = {dt_auc:.3f})\")\n",
    "plt.plot(rf_fpr, rf_tpr, label=f\"Random Forest (AUC = {rf_auc:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Random Guess\")\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve Comparison: Decision Tree vs Random Forest\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output, \"roc_dt_vs_rf.png\"), dpi=150)\n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e638750",
   "metadata": {},
   "source": [
    "---------------------\n",
    "## Dimensional reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea38db2",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c62eecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PCA (2 components)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(scaled_df)\n",
    "\n",
    "pc1_var = pca.explained_variance_ratio_[0] * 100\n",
    "pc2_var = pca.explained_variance_ratio_[1] * 100\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "scatter = plt.scatter(\n",
    "    X_pca[:, 0],\n",
    "    X_pca[:, 1],\n",
    "    c=y,\n",
    "    cmap=\"coolwarm\",\n",
    "    alpha=0.7)\n",
    "\n",
    "plt.xlabel(f\"PC1 ({pc1_var:.2f}% variance)\")\n",
    "plt.ylabel(f\"PC2 ({pc2_var:.2f}% variance)\")\n",
    "plt.title(\"PCA of Wine Quality Dataset\")\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=[\"Low Quality\", \"High Quality\"])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output, \"pca_plot_2d.png\"), dpi=150)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8eac05fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot 3D PCA\n",
    "# PCA with 3 components\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(scaled_df)\n",
    "\n",
    "pc1_var = pca.explained_variance_ratio_[0] * 100\n",
    "pc2_var = pca.explained_variance_ratio_[1] * 100\n",
    "pc3_var = pca.explained_variance_ratio_[2] * 100\n",
    "\n",
    "# Angles\n",
    "angles = [\n",
    "    (30, 60)\n",
    "]\n",
    "\n",
    "for idx, (elev, azim) in enumerate(angles):\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    scatter = ax.scatter(\n",
    "        X_pca[:, 0],\n",
    "        X_pca[:, 1],\n",
    "        X_pca[:, 2],\n",
    "        c=y,\n",
    "        cmap=\"coolwarm\",\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(f\"PC1 ({pc1_var:.2f}% variance)\")\n",
    "    ax.set_ylabel(f\"PC2 ({pc2_var:.2f}% variance)\")\n",
    "    ax.set_zlabel(f\"PC3 ({pc3_var:.2f}% variance)\")\n",
    "    ax.set_title(f\"3D PCA View {idx+1} (elev={elev}, azim={azim})\")\n",
    "\n",
    "    legend_elements = scatter.legend_elements()[0]\n",
    "    ax.legend(legend_elements, [\"Low Quality\", \"High Quality\"])\n",
    "\n",
    "    # Set the viewing angle\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(output, f\"pca_3d_view_{idx+1}.png\")\n",
    "    plt.savefig(save_path, dpi=150)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380e42c0",
   "metadata": {},
   "source": [
    "-------------------\n",
    "## Clustering\n",
    "### Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "229d664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_method = \"ward\"\n",
    "Z = linkage(scaled_df, method=\"ward\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "dendrogram(Z,truncate_mode=\"level\", p=5) #plot dendrogram only show 5-level\n",
    "plt.title(f\"Hierarchical Clustering Dendrogram ({linkage_method})\")\n",
    "plt.xlabel(\"Sample index or (cluster size)\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output, \"hierarchi_plot.png\"), dpi=150)\n",
    "#plt.show()\n",
    "plt.close()\n",
    "\n",
    "hc = AgglomerativeClustering(\n",
    "    n_clusters=2,\n",
    "    linkage=linkage_method,  # \"ward\"\n",
    "    metric=\"euclidean\"\n",
    ")\n",
    "cluster_labels = hc.fit_predict(scaled_df)\n",
    "\n",
    "# store the result \n",
    "df_cls[\"hc_cluster\"] = cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c702bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the clustering result on PCA\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(\n",
    "    X_pca[:, 0],\n",
    "    X_pca[:, 1],\n",
    "    c=df_cls[\"hc_cluster\"],\n",
    "    cmap=\"coolwarm\",\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA with Hierarchical Clustering Labels\")\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=[f\"Cluster {i}\" for i in sorted(df_cls[\"hc_cluster\"].unique())])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output, \"hierarchi_pca.png\"), dpi=150)\n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "633e6b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI: 0.058596317969862366\n",
      "NMI: 0.07017058115825778\n",
      "Purity: 0.6222639149468417\n"
     ]
    }
   ],
   "source": [
    "# Estimate the clustering results\n",
    "true_labels = df_cls[\"high_quality\"]\n",
    "cluster_labels = df_cls[\"hc_cluster\"]\n",
    "\n",
    "# Adjusted Rand Index\n",
    "ari = adjusted_rand_score(true_labels, cluster_labels)\n",
    "\n",
    "# Normalized Mutual Information\n",
    "nmi = normalized_mutual_info_score(true_labels, cluster_labels)\n",
    "\n",
    "# Purity\n",
    "crosstab = pd.crosstab(cluster_labels, true_labels)\n",
    "purity = np.sum(crosstab.max(axis=1)) / np.sum(crosstab.values)\n",
    "\n",
    "print(\"ARI:\", ari)\n",
    "print(\"NMI:\", nmi)\n",
    "print(\"Purity:\", purity)\n",
    "\n",
    "# Plot confusion matrix-like heatmap\n",
    "cm = pd.crosstab(cluster_labels, true_labels)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "\n",
    "plt.title(\"Cluster vs True Label\")\n",
    "plt.xlabel(\"True Label (High Quality 0/1)\")\n",
    "plt.ylabel(\"Cluster Label (HC Cluster)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output, \"hierarchi_heatmap.png\"), dpi=200)\n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec834a",
   "metadata": {},
   "source": [
    "### Clustering (Kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "76a088b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kmeans(df_scaled, k=2, random_state=42):\n",
    "    km = KMeans(n_clusters=k, n_init=\"auto\", random_state=random_state)\n",
    "    labels = km.fit_predict(df_scaled.values)\n",
    "\n",
    "    sil = np.nan\n",
    "    if k > 1 and len(df_scaled) > k:\n",
    "        try:\n",
    "            sil = silhouette_score(df_scaled.values, labels)\n",
    "        except Exception:\n",
    "            sil = np.nan\n",
    "\n",
    "    labels_series = pd.Series(labels, index=df_scaled.index, name=\"cluster\")\n",
    "    return km, labels_series, sil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd538ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using silhouette score to decide K\n",
    "silhouette_results = []\n",
    "\n",
    "for k in range(1, 6):\n",
    "    _, labels_k, sil = run_kmeans(scaled_df, k=k, random_state=42)\n",
    "    silhouette_results.append({\"k\": k, \"silhouette\": sil})\n",
    "\n",
    "silhouette_df = pd.DataFrame(silhouette_results)\n",
    "\n",
    "# plot the curve of the silhouette score\n",
    "plt.figure(figsize=(6, 4))\n",
    "mask = silhouette_df[\"k\"] > 1\n",
    "plt.plot(\n",
    "    silhouette_df.loc[mask, \"k\"],\n",
    "    silhouette_df.loc[mask, \"silhouette\"],\n",
    "    marker=\"o\"\n",
    ")\n",
    "plt.xlabel(\"Number of clusters (k)\")\n",
    "plt.ylabel(\"Silhouette score\")\n",
    "plt.title(\"Silhouette score for K-means (k = 2–5)\")\n",
    "plt.xticks([2, 3, 4, 5])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output, \"kmeans_silhouette_curve.png\"), dpi=150)\n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a05189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final results\n",
    "km, labels, silhouette = run_kmeans(scaled_df, k=2, random_state=42)\n",
    "df_cls[\"kmeans_cluster\"] = labels\n",
    "df_cls[\"kmeans_cluster\"] = 1 - df_cls[\"kmeans_cluster\"] # because the color is different from heirarchical results, so I reverse it.\n",
    "# Plot cluster result on PCA\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(\n",
    "    X_pca[:, 0],\n",
    "    X_pca[:, 1],\n",
    "    c=df_cls[\"kmeans_cluster\"],\n",
    "    cmap=\"coolwarm\",\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA with K-means Clustering Labels\")\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=[f\"Cluster {i}\" for i in sorted(df_cls[\"hc_cluster\"].unique())])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output, \"kmeans_pca.png\"), dpi=150)\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc5920a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI: 0.029125404082489644\n",
      "NMI: 0.029720560963749056\n",
      "Purity: 0.5866166353971232\n"
     ]
    }
   ],
   "source": [
    "true_labels = df_cls[\"high_quality\"]\n",
    "cluster_labels = df_cls[\"kmeans_cluster\"]\n",
    "\n",
    "# Adjusted Rand Index\n",
    "ari = adjusted_rand_score(true_labels, cluster_labels)\n",
    "\n",
    "# Normalized Mutual Information\n",
    "nmi = normalized_mutual_info_score(true_labels, cluster_labels)\n",
    "\n",
    "# Purity\n",
    "crosstab = pd.crosstab(cluster_labels, true_labels)\n",
    "purity = np.sum(crosstab.max(axis=1)) / np.sum(crosstab.values)\n",
    "\n",
    "print(\"ARI:\", ari)\n",
    "print(\"NMI:\", nmi)\n",
    "print(\"Purity:\", purity)\n",
    "\n",
    "# Plot confusion matrix-like heatmap\n",
    "cm = pd.crosstab(cluster_labels, true_labels)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "\n",
    "plt.title(\"Cluster vs True Label\")\n",
    "plt.xlabel(\"True Label (High Quality 0/1)\")\n",
    "plt.ylabel(\"Cluster Label (HC Cluster)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output, \"kmeans_heatmap.png\"), dpi=200)\n",
    "#plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c1_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
